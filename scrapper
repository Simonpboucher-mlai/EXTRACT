import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import numpy as np
import json
import os
from dotenv import load_dotenv
import pdfplumber
from io import BytesIO

# Charger les variables d'environnement depuis le fichier .env
load_dotenv()

# Fonction pour extraire le texte d'une page web ou d'un PDF
def extract_text(url):
    headers = {'User-Agent': 'Mozilla/5.0 (compatible; Bot/1.0)'}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()

        if url.lower().endswith('.pdf'):
            # Traitement pour les PDF
            pdf = pdfplumber.open(BytesIO(response.content))
            text = "\n".join(page.extract_text() for page in pdf.pages if page.extract_text())
            pdf.close()
            return text, None
        else:
            # Traitement pour les pages web
            try:
                soup = BeautifulSoup(response.content, 'lxml')
            except Exception:
                try:
                    soup = BeautifulSoup(response.content, 'html.parser')
                except Exception:
                    soup = BeautifulSoup(response.content, 'html5lib')

            for script in soup(["script", "style"]):
                script.extract()

            text = soup.get_text(separator=' ', strip=True)

        if not text:
            print(f"Le contenu de {url} est vide.")
        return text, soup

    except requests.exceptions.RequestException as e:
        print(f"Erreur lors de la récupération de {url}: {e}")
        return "", None
    except Exception as e:
        print(f"Erreur générale lors du traitement de {url}: {e}")
        return "", None

# Fonction pour diviser un texte en chunks
def split_text_into_chunks(text, max_tokens=800):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(current_chunk) >= max_tokens:
            chunks.append(" ".join(current_chunk))
            current_chunk = []

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

# Fonction pour générer des embeddings avec OpenAI en batch
def generate_openai_embedding(api_key, texts):
    url = "https://api.openai.com/v1/embeddings"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "input": texts,
        "model": "text-embedding-ada-002"
    }

    try:
        response = requests.post(url, headers=headers, json=data)
        response.raise_for_status()
        response_json = response.json()

        if 'data' in response_json:
            embeddings = [item["embedding"] for item in response_json["data"]]
            return embeddings
        else:
            print("Erreur: La clé 'data' est absente de la réponse.")
            return None
    except requests.exceptions.HTTPError as http_err:
        print(f"Erreur HTTP: {http_err}")
        print(f"Réponse de l'API: {response.text}")
    except Exception as err:
        print(f"Erreur lors de la requête : {err}")

    return None

# Fonction pour crawler un site web et traiter les PDFs trouvés
def crawl_and_process(start_url, max_pages):
    visited = set()
    to_visit = [start_url]
    texts = []
    pdf_urls = set()

    while to_visit and len(visited) < max_pages:
        url = to_visit.pop(0)
        if url not in visited:
            print(f"Traitement de {url}")
            visited.add(url)
            text, soup = extract_text(url)
            if text:
                texts.append(text)

            if soup:
                for link in soup.find_all('a', href=True):
                    new_url = urljoin(url, link['href'])
                    if new_url.lower().endswith('.pdf'):
                        pdf_urls.add(new_url)
                    elif new_url.startswith(start_url) and new_url not in visited:
                        to_visit.append(new_url)

            time.sleep(1)  # Pause pour être poli avec le serveur

    # Traiter les PDFs trouvés
    for pdf_url in pdf_urls:
        print(f"Traitement du PDF: {pdf_url}")
        pdf_text, _ = extract_text(pdf_url)
        if pdf_text:
            texts.append(pdf_text)

    return texts

# Fonction pour traiter, diviser en chunks, générer les embeddings, et sauvegarder les résultats
def process_and_save_embeddings(sites_info):
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("Erreur: La clé API OpenAI n'a pas été trouvée dans le fichier .env.")
        return

    all_embeddings = []
    all_chunks = []

    # Traiter les sites web
    for site_url, max_pages in sites_info.items():
        print(f"\nTraitement du site: {site_url} avec un maximum de {max_pages} pages.")
        texts = crawl_and_process(site_url, max_pages)
        combined_text = "\n\n".join(texts)
        chunks = split_text_into_chunks(combined_text)
        all_chunks.extend(chunks)

        # Générer les embeddings en batch
        embeddings = []
        batch_size = 10

        for i in range(0, len(chunks), batch_size):
            batch_chunks = chunks[i:i + batch_size]
            batch_embeddings = generate_openai_embedding(api_key, batch_chunks)
            if batch_embeddings is not None:
                embeddings.extend(batch_embeddings)

        all_embeddings.extend(embeddings)

    # Sauvegarder les résultats
    np.save("embeddings.npy", np.array(all_embeddings))
    with open("chunks.json", "w", encoding='utf-8') as f:
        json.dump(all_chunks, f, ensure_ascii=False, indent=2)

    print("\nEmbeddings et chunks sauvegardés avec succès.")

# Exemple d'utilisation
if __name__ == "__main__":
    sites_info = {
        "https://www.fsa.ulaval.ca": 2000  # Site web avec max 1000 pages
    }

    process_and_save_embeddings(sites_info)
